{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "from time import time\n",
    "import cv2\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['GORDON_REPO_DIR'], 'utilities'))\n",
    "from utilities2015 import *\n",
    "\n",
    "from matplotlib.path import Path\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.ndimage as nd\n",
    "import scipy\n",
    "\n",
    "from skimage.feature import hog\n",
    "from skimage import data, color, exposure\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    classes = ['12N', '5N', '7n', '7N', 'Gr', 'LVe', 'Pn', 'SuVe', 'VLL', 'Bg'];\n",
    "    def __init__(self, training_X, clf, vocabulary):\n",
    "        self.training_X = training_X;\n",
    "        self.clf = clf;\n",
    "        self.vocabulary = vocabulary;\n",
    "    \n",
    "    def generateKernel(self, test_X):\n",
    "        n_testing = test_X.shape[0];\n",
    "        n_training = self.training_X.shape[0];\n",
    "        testKernel = np.zeros(shape=(n_testing, n_training));\n",
    "        for i in range(0, n_testing):\n",
    "            for j in range(0, n_training):\n",
    "                testKernel[i][j] = sum(np.minimum(test_X[i], self.training_X[j]));\n",
    "        return testKernel;\n",
    "    \n",
    "    def getHistogram(self, image, kmeans, M, L):\n",
    "        x_size = image.shape[1]; # Column\n",
    "        y_size = image.shape[0]; # Row\n",
    "        # Get SIFT key points and descriptor.\n",
    "        sift = cv2.SIFT();\n",
    "        key_points, descriptor = sift.detectAndCompute(image, None);\n",
    "        # Map key points with type.\n",
    "        if descriptor is None:\n",
    "            return None;\n",
    "        cluster_labels = kmeans.predict(descriptor);\n",
    "        cluster_labels = cluster_labels;\n",
    "        # Generate position list of key_points\n",
    "        positions = np.asarray([kp.pt for kp in key_points], np.int)\n",
    "        # Level 0\n",
    "        weight_0 = 1.0 / (2**L);\n",
    "        histogram = np.bincount(cluster_labels, minlength = M);\n",
    "        histogram = histogram * weight_0;\n",
    "        # Other levels\n",
    "        for l in range(1, L):\n",
    "            weight = 1.0 / (2**(L - l + 1));\n",
    "            grid_size_x = x_size / (2**l);\n",
    "            grid_size_y = y_size / (2**l);\n",
    "            grid_boundaries_x = range(0, x_size, grid_size_x);\n",
    "            grid_boundaries_y = range(0, y_size, grid_size_y);\n",
    "            number_of_grid = (2**l)**2;\n",
    "            grid_crs = positions / [grid_size_x, grid_size_y];\n",
    "            grid_cols = grid_crs[:,0];  # x\n",
    "            grid_rows = grid_crs[:,1];  # y\n",
    "    \n",
    "            keypoint_grid_indices = grid_rows * (2**l) + grid_cols;\n",
    "            hists = np.zeros(shape=(number_of_grid, M), dtype=np.int);\n",
    "            for ki, label in zip(keypoint_grid_indices, cluster_labels):\n",
    "                hists[ki][label] += 1;\n",
    "            hists = hists * weight;\n",
    "            histogram = np.hstack((histogram, hists.flatten()));\n",
    "        return (histogram / len(key_points)); # Normalization\n",
    "\n",
    "    def predictLabel(self, image):\n",
    "        M = self.vocabulary.shape[0];\n",
    "        L = 3;\n",
    "        kmeans = KMeans(init='random', n_clusters=M, n_init=10);\n",
    "        kmeans.fit(self.vocabulary);\n",
    "        img = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY);\n",
    "        histogram = self.getHistogram(img, kmeans, M, L);\n",
    "        testKernel = self.generateKernel(np.asarray(histogram).reshape(1, histogram.shape[0]));\n",
    "        return Classifier.classes[self.clf.predict(testKernel)[0]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patches = np.load('/oasis/projects/nsf/csd395/yuncong/CSHL_data_patches/patches/patches_Pn_138.npy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myClassifier = pickle.load(open('/oasis/projects/nsf/csd395/ruogu/svm/classifier.p', \"rb\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pn\n"
     ]
    }
   ],
   "source": [
    "print myClassifier.predictLabel(patches[10]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
