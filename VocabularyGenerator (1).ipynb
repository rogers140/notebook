{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['GORDON_REPO_DIR'], 'utilities'))\n",
    "from utilities2015 import *\n",
    "\n",
    "from matplotlib.path import Path\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.ndimage as nd\n",
    "import scipy\n",
    "\n",
    "from skimage.feature import hog\n",
    "from skimage import data, color, exposure\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return the histogram of an image by applying SIFT + K-means + SPM\n",
    "# image: the input grayscaled image\n",
    "# kmeans: python kmeans object used to predict\n",
    "#             feature type of a new sift descriptor\n",
    "# M: number of channels (feature typs)\n",
    "# L: number of SPM levels.\n",
    "def getHistogram(image, kmeans, M, L):\n",
    "    x_size = image.shape[0];\n",
    "    y_size = image.shape[1];\n",
    "    # Get SIFT key points and descriptor.\n",
    "    sift = cv2.SIFT();\n",
    "    key_point, descriptor = sift.detectAndCompute(image, None);\n",
    "    # Map key points with type.\n",
    "    cluster_labels = kmeans.predict(descriptor);\n",
    "    \n",
    "    # Level 0\n",
    "    histogram = np.zeros(M);\n",
    "    weight_0 = 1.0 / (2**L);\n",
    "    for k in range(0, len(key_point)):\n",
    "        histogram[cluster_labels[k]] += 1;\n",
    "    histogram = histogram * weight_0;\n",
    "    # Other levels\n",
    "    for l in range(1, L):\n",
    "        grid_x = 1.0 * x_size / (2**l);\n",
    "        grid_y = 1.0 * y_size / (2**l);\n",
    "        weight = 1.0 / (2**(L - l + 1));\n",
    "        for i in range(0, (2**l)):\n",
    "            x_min = i * grid_x;\n",
    "            x_max = x_min + grid_x;\n",
    "            for j in range(0, (2**l)):\n",
    "                y_min = j * grid_y;\n",
    "                y_max = y_min + grid_y;\n",
    "                histogram_tmp = np.zeros(M);\n",
    "                # Count and update histogram from this grid.\n",
    "                for k in range(0, len(key_point)):\n",
    "                    if (x_min <= key_point[k].pt[0] <= x_max and\n",
    "                        y_min <= key_point[k].pt[1] <= y_max):\n",
    "                        histogram_tmp[cluster_labels[k]] += 1;\n",
    "                histogram_tmp = histogram_tmp * weight;\n",
    "                histogram = np.hstack((histogram, histogram_tmp));\n",
    "    return (histogram / np.sum(histogram)); # Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return the kmeans object (vocabulary)\n",
    "# patches: the training gray image set\n",
    "# M: size of feature types\n",
    "# sampleNumber: number of small samples from each patch image, default=8\n",
    "# samplePatchSize: length of the square sample, default 16\n",
    "def getVocabulary(imageSet, M, sampleNumber=8, samplePatchSize=16):\n",
    "    descriptor_bank = None;\n",
    "    for img in imageSet:\n",
    "        # Choose 8 random 16*16 patches from img\n",
    "        for i in range(0, sampleNumber):\n",
    "            x = random.randint(0, img.shape[0] - samplePatchSize);\n",
    "            y = random.randint(0, img.shape[1] - samplePatchSize);\n",
    "            currentImg = img[x:x+samplePatchSize, y:y+samplePatchSize];\n",
    "            # Get sift descriptor\n",
    "            sift = cv2.SIFT();\n",
    "            key_point, descriptor = sift.detectAndCompute(currentImg, None);\n",
    "            if descriptor is None:\n",
    "                continue;\n",
    "            # Add descriptor to descriptor_bank\n",
    "            if descriptor_bank is None:\n",
    "                descriptor_bank = descriptor;\n",
    "            else:\n",
    "                descriptor_bank = np.vstack((descriptor_bank, descriptor));\n",
    "    print \"Total number of descriptors: {}\".format(descriptor_bank.shape[0]);\n",
    "    # Doing K-means to build vocabulary\n",
    "    vocabulary = KMeans(init='random', n_clusters=M, n_init=10);\n",
    "    vocabulary.fit(descriptor_bank);\n",
    "    return vocabulary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingDir = '/oasis/projects/nsf/csd395/yuncong/CSHL_data_patches/patches/';\n",
    "savingFileName = '/oasis/projects/nsf/csd395/ruogu/vocabulary/vocabulary.npy';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of image patches in training set: 193797\n"
     ]
    }
   ],
   "source": [
    "# Load data and convert to grayscale\n",
    "trainingImages = [];\n",
    "fileNames = os.listdir(trainingDir);\n",
    "for fileName in fileNames:\n",
    "    # print fileName;\n",
    "    images = np.load(trainingDir + fileName);\n",
    "    for img in images:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY);\n",
    "        trainingImages.append(img);\n",
    "print \"Number of image patches in training set: {}\".format(len(trainingImages));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingSamples = random.sample(trainingImages, 10000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of descriptors: 29321\n"
     ]
    }
   ],
   "source": [
    "M = 200; # vocabulary size\n",
    "vocabulary = getVocabulary(trainingSamples, M);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save vocabulary\n",
    "np.save(savingFileName, vocabulary.cluster_centers_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
